This article is going to review some of the popular resources for learning robotics. For fair comparison, we compare a basic concept that is common to all these resources: the concept of representing pose. 

Let's start with Modern Robotics. This resource is a package consisting of a book, a Coursera course, a YouTube playlist, a wiki and a software brought to you by Northwestern University. The Fundamentals of Robotics Course by Mecharithm uses this book as reference. 
All reference frames are right-handed, as illustrated in Modern Robotics. A positive rotation about an axis is defined as the direction in which the fingers of the right hand curl when the thumb is pointed along the axis. 

Transformation matrix is a way of representing pose. A few things anyone would know once they understand transformation matrix are:
1. Changing the reference frame of a vector or a frame.
2. Inverting a homogeneous transformation matrix.
3. Merging rotation and translation to transformation. 

Modern Robotics covers the above topics very clearly. Corke does not do a good job here:
1. They discuss composition of transformation matrix in 2.3.2.1 but do not explain why we would want to multiply the matrices to compose another. They demonstrate composition of transforms by `T=transl(2,0,0)@trotx(pi/2)@transl(0,1,0)`. Now `transl(2,0,0)`, `trotx(pi/2)` and `transl(0,1,0)` return three different transformation matrices: why use three transformation matrices T1, T2, T3 to demonstrate composition which is simply T1T2? Well, actually, in their demonstration they treat `transl(2,0,0)@trotx(pi/2)` as T1 and `transl(0,1,0)` as T2 and they do not explicitly mention but assume the reader to just figure out that in T1, `trotx(pi/2)` contributes to the rotation and `transl(2,0,0)` contributes to the translation part of the transformation.
2. They discuss the inverse of a transformation matrix, yes, so how can we get inverse of a transformation matrix using their toolbox? By using `.inv()` on an SE3 object. SE3 object? So, when they say that `T=transl(2,0,0)@trotx(pi/2)@transl(0,1,0)` gives us a transformation matrix, do they mean a numpy array or an se3 object that encapsulates a numpy array? Well, things we won't know from reading their book but by installing their toolbox and figuring it out ourselves. 
3. Given a transformation `T` the rotation and translation components can be obtained using `t2r(T)` and `transl(T)` from their toolbox respectively. But either their toolbox does not have or their book does not document a function to get transformation, given its rotation and translation components. 

From a surface level, due to the fact that Corke uses more coloful images and Modern Robotics uses more dry-academic-formal English, it may seem that Corke is more introductory than Modern Robotics, but once you start reading, Corke will be expecting you to read between the lines while Modern Robotics will be the one babying you.

Now, compared to colorful Corke, Handbook and Modern Robotics both look black-and-white. First let's compare Modern Robotics with Handbook on the same three points. Now if we compare them on the basis of how-to-questions: "how to invert a homogenous transformation?" or "how to merge rotation and translation to transformation?", Modern Robotics would be a clear winner because, come on, dude implemented these How-Tos in their software, that too in pure language; Modern Robotics has a pure Python and a pure Matlab version of the same software unlike Corke that has a mixture of Python and C++ in their toolbox; on the other hand Handbook does not concern itself with programming at all. But hey, let's not throw-away the book like that, let me give you a perspective where Handbook wins over Modern Robotics. Modern Robotics introduces transformation matrix in this manner:
> Rather than identifying R and p separately, we package them into a single matrix as follows.

Why you package them into a single matrix? "Package" has a positive connotation that before it was all over the place but now it is packaged to one thing. Still that's very implicit. Handbook does a great job in answering the question and I quote
>Homogeneous transformations are particularly attractive when compact notation is desired and/or when ease of programming is the most important consideration. This is not, however, a computationally efficient representation since it introduces a large number of additional multiplications by ones and zeros. Although homogeneous transformation matrices technically contain 16 elements, four are defined to be zero or one, and the remaining elements are composed of a rotation matrix and a position vector.

I think it not only answers the question but also tells you why or when to use homogenous transformations. Modern Robotics expects you to know what exactly you are looking for and is in no mood in attracting you to learn from them, whereas Handbook sparks an interest in you and then starts teaching you. 

At this point of reading this review it may seem that Corke is probably the worst and Modern Robotics is the best for learning robotics, but it will be unfair to finish this article here because Modern Robotics has the word "Mechanics" in its title whereas Corke has "Vision" in its title and this whole time we judged these 3 books based on only mechanics. Comparing on the basis of computer vision is going to be an easy win for Corke because the other two do not even enough contents relevant to vision. Still, let's discuss about their vision part without comparing. 

To demonstrate reading image from files, they use the function `iread` from their toolbox. Why? Is `cv2.imread` that bad? It's not even that they are providing alternative to cv2, their toolbox is largely based on OpenCV. I like their "excurses" - interesting to read but not their focus - that why they call it "excurses". The image object from their toolbox can be cropped using `roi` method. Again, why? By the way "roi" simply is an abbreviation for "region of interest" not a fancy scientific word. The function takes four arguments which is obvious for cropping, but why not crop by simply slicing the numpy array? Wouldn't it help convince the reader that an image is nothing but a numpy array? To demonstrate Otsu threshold, they start with plotting the histogram of a grayscale image. The histogram has two clearly defined peaks, a bimodal distribution, which correspond to two populations of pixels. As we all can guess, one population corresponds to foreground, another to background. The optimal threshold can be computed using Otsu’s method. Let me ask a less intuitive question: what if there is only one tall peak in a histogram? Could Otsu’s method simply do the magic? You won't find this answer in the book; by the way the answer is "it won't - but you can make some tweaks and it will"; I will write a separate article on this. In the section about lightfield cameras they claim that the raw image from the sensor array can be decoded in to a 4-dimensional light field. How? Now you are allowed to demonstrate a function from your toolbox but in the section there is no piece of code at all. Oh that's right, OpenCV also does not have any function that does this. So let me get this straight - the point the toolbox is to make OpenCV functions, which themselves are abstraction of computer vision algorithms, more abstract? Learning robotics and learning toolbox are going to be two separate challenges - the toolbox is not to ease your learning experience. 